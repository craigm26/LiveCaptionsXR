/// Model representing a visually detected object with accessibility context
/// 
/// This model demonstrates how we structure visual data for Gemma 3n integration,
/// providing spatial and contextual information crucial for accessibility applications.
/// 
/// For Google Gemma 3n Hackathon: Shows comprehensive visual object modeling
/// that supports multimodal understanding and spatial reasoning.
import 'dart:ui';

class VisualObject {
  /// Object label/category (e.g., 'microwave', 'door', 'person')
  final String label;
  
  /// Confidence score from 0.0 to 1.0 from vision model
  final double confidence;
  
  /// Bounding box coordinates in image space
  final Rect boundingBox;
  
  /// Human-readable description for accessibility
  /// Generated by Gemma 3n's multimodal understanding
  final String description;
  
  /// Spatial relationship to user (e.g., 'to your left', 'in front of you')
  final String spatialRelation;
  
  /// Whether this object is relevant to current audio context
  final bool isRelevantToAudio;
  
  /// Object distance estimate (in meters, if available)
  final double? estimatedDistance;
  
  /// Additional metadata for enhanced accessibility features
  final Map<String, dynamic> metadata;
  
  /// World transform matrix (16 doubles, row-major 4x4)
  final List<double>? worldTransform;

  VisualObject({
    required this.label,
    required this.confidence,
    required this.boundingBox,
    this.description = '',
    this.spatialRelation = 'unknown',
    this.isRelevantToAudio = false,
    this.estimatedDistance,
    this.metadata = const {},
    this.worldTransform,
  });
  
  /// Create a copy with updated fields
  VisualObject copyWith({
    String? label,
    double? confidence,
    Rect? boundingBox,
    String? description,
    String? spatialRelation,
    bool? isRelevantToAudio,
    double? estimatedDistance,
    Map<String, dynamic>? metadata,
    List<double>? worldTransform,
  }) {
    return VisualObject(
      label: label ?? this.label,
      confidence: confidence ?? this.confidence,
      boundingBox: boundingBox ?? this.boundingBox,
      description: description ?? this.description,
      spatialRelation: spatialRelation ?? this.spatialRelation,
      isRelevantToAudio: isRelevantToAudio ?? this.isRelevantToAudio,
      estimatedDistance: estimatedDistance ?? this.estimatedDistance,
      metadata: metadata ?? this.metadata,
      worldTransform: worldTransform ?? this.worldTransform,
    );
  }
  
  /// Convert to JSON for storage or transmission
  Map<String, dynamic> toJson() {
    return {
      'label': label,
      'confidence': confidence,
      'boundingBox': {
        'left': boundingBox.left,
        'top': boundingBox.top,
        'right': boundingBox.right,
        'bottom': boundingBox.bottom,
      },
      'description': description,
      'spatialRelation': spatialRelation,
      'isRelevantToAudio': isRelevantToAudio,
      'estimatedDistance': estimatedDistance,
      'metadata': metadata,
      'worldTransform': worldTransform,
    };
  }
  
  /// Create from JSON
  factory VisualObject.fromJson(Map<String, dynamic> json) {
    final bbox = json['boundingBox'] as Map<String, dynamic>;
    return VisualObject(
      label: json['label'] as String,
      confidence: (json['confidence'] as num).toDouble(),
      boundingBox: Rect.fromLTRB(
        (bbox['left'] as num).toDouble(),
        (bbox['top'] as num).toDouble(),
        (bbox['right'] as num).toDouble(),
        (bbox['bottom'] as num).toDouble(),
      ),
      description: json['description'] as String? ?? '',
      spatialRelation: json['spatialRelation'] as String? ?? 'unknown',
      isRelevantToAudio: json['isRelevantToAudio'] as bool? ?? false,
      estimatedDistance: (json['estimatedDistance'] as num?)?.toDouble(),
      metadata: Map<String, dynamic>.from(json['metadata'] as Map? ?? {}),
      worldTransform: (json['worldTransform'] as List<dynamic>?)?.map((e) => (e as num).toDouble()).toList(),
    );
  }
  
  /// Human-readable string representation
  @override
  String toString() {
    return 'VisualObject(label: $label, confidence: ${confidence.toStringAsFixed(2)}, '
           'relation: $spatialRelation, audioRelevant: $isRelevantToAudio, worldTransform: ${worldTransform != null ? '[${worldTransform!.take(4).map((e) => e.toStringAsFixed(2)).join(', ')} ...]' : 'null'})';
  }
  
  /// Check if confidence is high enough for reliable identification
  bool get isReliable => confidence >= 0.7;
  
  /// Get center point of bounding box
  Offset get center => Offset(
    boundingBox.left + boundingBox.width / 2,
    boundingBox.top + boundingBox.height / 2,
  );
  
  /// Get object area in pixels
  double get area => boundingBox.width * boundingBox.height;
  
  /// Check if this object is likely a sound source
  bool get isPotentialSoundSource {
    const soundSourceLabels = [
      'microwave', 'timer', 'phone', 'doorbell', 'alarm',
      'television', 'radio', 'speaker', 'person', 'baby',
      'dog', 'cat', 'car', 'motorcycle', 'siren'
    ];
    
    return soundSourceLabels.any((source) => 
      label.toLowerCase().contains(source));
  }
  
  /// Get accessibility description combining visual and spatial information
  String get accessibilityDescription {
    final buffer = StringBuffer();
    
    buffer.write(label);
    
    if (description.isNotEmpty) {
      buffer.write(' - $description');
    }
    
    if (spatialRelation != 'unknown') {
      buffer.write(' ($spatialRelation)');
    }
    
    if (estimatedDistance != null) {
      buffer.write(' at ${estimatedDistance!.toStringAsFixed(1)}m');
    }
    
    return buffer.toString();
  }
} 