import Flutter
import UIKit
import MediaPipeTasksGenAI

public class Gemma3nMultimodalPlugin: NSObject, FlutterPlugin, FlutterStreamHandler {
  private var llmInference: MediaPipeTasksGenAI.LlmInference?
  private var eventSink: FlutterEventSink?
  private var bufferedAudio: [[Float]] = []
  private var registrar: FlutterPluginRegistrar?
  
  // Audio capture state
  private var isCapturingAudio = false
  private var audioBuffer: [Float] = []
  private let maxBufferSize = 16000 * 2 // 2 seconds at 16kHz
  
  // Configurable speech processing parameters
  private var voiceActivityThreshold: Float = 0.01
  private var finalResultThreshold: Float = 0.005
  private var bufferSizeMs: Int = 2000
  private var interimResultIntervalMs: Int = 1000
  private var finalResultIntervalMs: Int = 3000
  private var currentLanguage: String = "en"
  private var enableLanguageDetection: Bool = false
  private var enableRealTimeEnhancement: Bool = true
  
  public static func register(with registrar: FlutterPluginRegistrar) {
    let channel = FlutterMethodChannel(name: "gemma3n_multimodal", binaryMessenger: registrar.messenger())
    let stream = FlutterEventChannel(name: "gemma3n_multimodal_stream", binaryMessenger: registrar.messenger())
    let instance = Gemma3nMultimodalPlugin()
    instance.registrar = registrar
    registrar.addMethodCallDelegate(instance, channel: channel)
    stream.setStreamHandler(instance)
  }

  // MARK: - Helper Methods for Bundle Asset Handling
  
  /// Resolves model path from Flutter assets or validates external path
  private func resolveModelPath(_ path: String) -> String? {
    // If path is absolute, verify file exists
    if path.hasPrefix("/") || path.hasPrefix("file://") {
      if FileManager.default.fileExists(atPath: path) {
        return path
      }
      return nil
    }
    
    // For Flutter assets, use the registrar to lookup the asset
    guard let registrar = self.registrar else {
      print("âŒ No registrar available for asset lookup")
      return nil
    }
    
    // Try to find the asset using Flutter's asset lookup
    let assetKey = registrar.lookupKey(forAsset: path)
    if let assetPath = Bundle.main.path(forResource: assetKey, ofType: nil) {
      print("âœ… Found Flutter asset at: \(assetPath)")
      return assetPath
    }
    
    // Fallback: Check if it's in the main bundle directly (for manual bundle inclusion)
    if let bundlePath = Bundle.main.path(forResource: path.replacingOccurrences(of: ".task", with: ""), ofType: "task") {
      print("âœ… Found bundled model at: \(bundlePath)")
      return bundlePath
    }
    
    // Additional fallback for assets/models/ prefix
    let assetsPath = "assets/models/\(path)"
    let assetsKey = registrar.lookupKey(forAsset: assetsPath)
    if let assetPath = Bundle.main.path(forResource: assetsKey, ofType: nil) {
      print("âœ… Found Flutter asset with assets/models/ prefix at: \(assetPath)")
      return assetPath
    }
    
    print("âŒ Model file not found. Searched paths: [\(path), \(assetKey), \(assetsKey)]")
    return nil
  }
  
  /// Creates properly configured LLM Inference Options for MediaPipe
  private func createLlmOptions(modelPath: String) -> MediaPipeTasksGenAI.LlmInference.Options {
    let options = MediaPipeTasksGenAI.LlmInference.Options(modelPath: modelPath)
    options.maxTokens = 1000
    return options
  }
  
  /// Creates session options with proper parameter configuration
  private func createSessionOptions(_ args: [String: Any]? = nil) -> MediaPipeTasksGenAI.LlmInference.Session.Options {
    let sessionOptions = MediaPipeTasksGenAI.LlmInference.Session.Options()
    
    // Apply parameters from args or use defaults
    if let args = args {
      sessionOptions.topk = args["topK"] as? Int ?? 40
      sessionOptions.topp = args["topP"] as? Float ?? 0.9
      sessionOptions.temperature = args["temperature"] as? Float ?? 0.8
    } else {
      // Default values optimized for Gemma3n
      sessionOptions.topk = 40
      sessionOptions.topp = 0.9
      sessionOptions.temperature = 0.8
    }
    
    return sessionOptions
  }
  
  /// Validates system requirements and available memory
  private func validateSystemRequirements() -> (isValid: Bool, error: String?) {
    // Check iOS version (MediaPipe GenAI requires iOS 12.0+)
    if #available(iOS 12.0, *) {
      // Check available memory (basic check)
      let processInfo = ProcessInfo.processInfo
      if processInfo.physicalMemory < 1_000_000_000 { // 1GB minimum
        return (false, "Insufficient memory for model loading")
      }
      return (true, nil)
    } else {
      return (false, "iOS 12.0 or later required for MediaPipe GenAI")
    }
  }

  public func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {
    switch call.method {
    case "getPlatformVersion":
      result("iOS " + UIDevice.current.systemVersion)
    case "loadModel":
      guard let args = call.arguments as? [String: Any],
            let path = args["path"] as? String else {
        result(FlutterError(code: "INVALID_ARGUMENT", message: "Missing model path", details: nil))
        return
      }
      
      // Validate system requirements first
      let validation = validateSystemRequirements()
      if !validation.isValid {
        result(FlutterError(code: "SYSTEM_REQUIREMENTS", message: validation.error ?? "System requirements not met", details: nil))
        return
      }
      
      // Resolve model path using iOS bundle if needed
      guard let resolvedPath = resolveModelPath(path) else {
        result(FlutterError(code: "MODEL_NOT_FOUND", message: "Model file not found at path: \(path)", details: [
          "searchedPaths": [
            path,
            "Bundle.main/\(path)",
            "Bundle.main/assets/models/\(path)"
          ]
        ]))
        return
      }
      
      let useANE = args["useANE"] as? Bool ?? true
      let useGPU = args["useGPU"] as? Bool ?? false
      
      do {
        // Create LLM Inference Options with proper iOS bundle path
        let options = createLlmOptions(modelPath: resolvedPath)
        
        // Configure additional generation parameters
        options.maxTokens = args["maxTokens"] as? Int ?? 1000
        
        // Configure generation parameters
        if let topK = args["topK"] as? Int {
          // These will be applied at session level
        }
        if let temperature = args["temperature"] as? Float {
          // These will be applied at session level  
        }
        
        // Initialize LLM Inference
        llmInference = try MediaPipeTasksGenAI.LlmInference(options: options)
        
        // Log successful loading for debugging
        print("âœ… Model loaded successfully from: \(resolvedPath)")
        print("ðŸ”§ Hardware acceleration - ANE: \(useANE), GPU: \(useGPU)")
        
        result([
          "success": true,
          "modelPath": resolvedPath,
          "useANE": useANE,
          "useGPU": useGPU
        ])
      } catch {
        llmInference = nil
        let errorMessage = "Failed to load model from \(resolvedPath): \(error.localizedDescription)"
        print("âŒ \(errorMessage)")
        result(FlutterError(code: "LOAD_FAILED", message: errorMessage, details: [
          "originalPath": path,
          "resolvedPath": resolvedPath,
          "error": error.localizedDescription
        ]))
      }
    case "unloadModel":
      llmInference = nil
      bufferedAudio.removeAll()
      result(nil)
    case "isModelLoaded":
      result(llmInference != nil)
    case "getModelInfo":
      if let llm = llmInference {
        result([
          "isLoaded": true,
          "modelType": "Gemma3n",
          "capabilities": ["text", "audio", "image"]
        ])
      } else {
        result([
          "isLoaded": false
        ])
      }
    case "getBundleModelPaths":
      // Helper method to discover available .task files in bundle
      var bundleModels: [String] = []
      
      // Search in main bundle
      let bundlePath = Bundle.main.bundlePath
      let fileManager = FileManager.default
      do {
        let contents = try fileManager.contentsOfDirectory(atPath: bundlePath)
        bundleModels.append(contentsOf: contents.filter { $0.hasSuffix(".task") })
        
        // Also search in assets/models if it exists
        let assetsPath = "\(bundlePath)/assets/models"
        if fileManager.fileExists(atPath: assetsPath) {
          let assetsContents = try fileManager.contentsOfDirectory(atPath: assetsPath)
          bundleModels.append(contentsOf: assetsContents.filter { $0.hasSuffix(".task") }.map { "assets/models/\($0)" })
        }
      } catch {
        print("Warning: Could not scan bundle for .task files: \(error)")
      }
      
      result([
        "bundleModels": bundleModels,
        "bundlePath": Bundle.main.bundlePath
      ])
    case "transcribeAudio":
      guard let args = call.arguments as? [String: Any],
            let audio = args["audio"] as? FlutterStandardTypedData,
            let llm = llmInference else {
        result(FlutterError(code: "INVALID_ARGUMENT", message: "Missing audio or model not loaded", details: nil))
        return
      }
      let floats = convertPcm16ToFloat32(audio.data)
      do {
        // For now, use basic text generation as MediaPipe iOS doesn't expose audio API directly
        let audioDescription = "Audio transcription request with \(floats.count) samples"
        let sessionOptions = createSessionOptions(args)
        let session = try MediaPipeTasksGenAI.LlmInference.Session(llmInference: llm, options: sessionOptions)
        try session.addQueryChunk(inputText: audioDescription)
        let transcription = try session.generateResponse()
        result(transcription)
      } catch {
        result(FlutterError(code: "INFERENCE_FAILED", message: error.localizedDescription, details: nil))
      }
    case "runMultimodal":
      guard let llm = llmInference else {
        result(FlutterError(code: "NOT_READY", message: "Model not loaded", details: nil))
        return
      }
      let args = call.arguments as? [String: Any] ?? [:]
      do {
        // For now, use basic text generation as MediaPipe iOS doesn't expose full multimodal API
        var prompt = "Multimodal inference request: "
        if let text = args["text"] as? String {
          prompt += text
        }
        if let imgData = args["image"] as? FlutterStandardTypedData {
          prompt += " [Image data: \(imgData.data.count) bytes]"
        }
        if let audioData = args["audio"] as? FlutterStandardTypedData {
          let floats = convertPcm16ToFloat32(audioData.data)
          prompt += " [Audio data: \(floats.count) samples]"
        }
        
        let sessionOptions = createSessionOptions(args)
        let session = try MediaPipeTasksGenAI.LlmInference.Session(llmInference: llm, options: sessionOptions)
        try session.addQueryChunk(inputText: prompt)
        let response = try session.generateResponse()
        result(response)
      } catch {
        result(FlutterError(code: "INFERENCE_FAILED", message: error.localizedDescription, details: nil))
      }
    case "addAudioChunk":
      guard let args = call.arguments as? [String: Any],
            let audio = args["audio"] as? FlutterStandardTypedData else {
        result(FlutterError(code: "INVALID_ARGUMENT", message: "Missing audio", details: nil))
        return
      }
      let floats = convertPcm16ToFloat32(audio.data)
      bufferedAudio.append(floats)
      result(nil)
    case "startAudioCapture":
      startAudioCapture(call, result: result)
    case "stopAudioCapture":
      stopAudioCapture(result)
    case "processAudioChunk":
      processAudioChunk(call, result: result)
    case "updateConfig":
      updateSpeechConfig(call, result: result)
    case "generateText":
      generateText(call, result: result)
    case "getASRCapabilities":
      getASRCapabilities(result)
    default:
      result(FlutterMethodNotImplemented)
    }
  }

  // MARK: - FlutterStreamHandler
  public func onListen(withArguments arguments: Any?, eventSink events: @escaping FlutterEventSink) -> FlutterError? {
    print("ðŸ”„ Setting up event stream...")
    eventSink = events
    
    // Handle case where no arguments are provided (default to transcription)
    if arguments == nil {
      print("âœ… Default stream setup completed")
      return nil
    }
    
    guard let args = arguments as? [String: Any] else {
      print("âŒ Invalid stream arguments format")
      return FlutterError(code: "INVALID_ARGUMENT", message: "Invalid arguments format", details: nil)
    }
    
    let type = args["type"] as? String ?? "transcription"
    print("ðŸŽ¯ Setting up stream for type: \(type)")
    
    if type == "transcription" {
      handleStreamTranscription(args)
    } else if type == "multimodal" {
      handleStreamMultimodal(args)
    } else {
      print("âŒ Unknown stream type: \(type)")
      return FlutterError(code: "INVALID_ARGUMENT", message: "Unknown stream type: \(type)", details: nil)
    }
    print("âœ… Stream setup completed for type: \(type)")
    return nil
  }

  public func onCancel(withArguments arguments: Any?) -> FlutterError? {
    print("ðŸ›‘ Cancelling event stream")
    eventSink = nil
    return nil
  }

  private func handleStreamTranscription(_ args: [String: Any]) {
    // Just verify that the model is loaded - don't require audio data at stream setup
    guard let llm = llmInference else {
      eventSink?(FlutterError(code: "NOT_READY", message: "Model not loaded", details: nil))
      return
    }
    
    // Apply configuration if provided
    if let config = args["config"] as? [String: Any] {
      updateConfigFromMap(config)
    }
    
    // Stream setup successful - the actual transcription will happen in processAudioChunk
    print("âœ… Transcription stream set up successfully with config")
  }

  private func handleStreamMultimodal(_ args: [String: Any]) {
    guard let llm = llmInference else {
      eventSink?(FlutterError(code: "NOT_READY", message: "Model not loaded", details: nil))
      return
    }
    do {
      // Build multimodal prompt
      var prompt = "Stream multimodal request: "
      if let text = args["text"] as? String {
        prompt += text
      }
      if let imgData = args["image"] as? FlutterStandardTypedData {
        prompt += " [Image data: \(imgData.data.count) bytes]"
      }
      if let audioData = args["audio"] as? FlutterStandardTypedData {
        let floats = convertPcm16ToFloat32(audioData.data)
        prompt += " [Audio data: \(floats.count) samples]"
      }
      
      let sessionOptions = createSessionOptions(args)
      let session = try MediaPipeTasksGenAI.LlmInference.Session(llmInference: llm, options: sessionOptions)
      try session.addQueryChunk(inputText: prompt)
      let resultStream = session.generateResponseAsync()
      
      Task {
        do {
          for try await partialResult in resultStream {
            DispatchQueue.main.async {
              self.eventSink?(partialResult)
            }
          }
          DispatchQueue.main.async {
            self.eventSink?(FlutterEndOfEventStream)
          }
        } catch {
          DispatchQueue.main.async {
            self.eventSink?(FlutterError(code: "STREAM_FAILED", message: error.localizedDescription, details: nil))
          }
        }
      }
    } catch {
      eventSink?(FlutterError(code: "STREAM_FAILED", message: error.localizedDescription, details: nil))
    }
  }

  private func convertPcm16ToFloat32(_ data: Data) -> [Float] {
    var floats = [Float](repeating: 0.0, count: data.count / 2)
    data.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) in
      let int16Ptr = ptr.bindMemory(to: Int16.self)
      for i in 0..<floats.count {
        floats[i] = Float(int16Ptr[i]) / Float(Int16.max)
      }
    }
    return floats
  }
  
  /// Calculate RMS audio level for voice activity detection
  private func calculateAudioLevel(_ audioSamples: [Float]) -> Float {
    guard !audioSamples.isEmpty else { return 0.0 }
    
    let sumOfSquares = audioSamples.reduce(0.0) { sum, sample in
      sum + (sample * sample)
    }
    
    let rms = sqrt(sumOfSquares / Float(audioSamples.count))
    return rms
  }
  
  // MARK: - Audio Capture Methods
  
  private func startAudioCapture(_ call: FlutterMethodCall, result: @escaping FlutterResult) {
    guard llmInference != nil else {
      print("âŒ Audio capture failed - model not loaded")
      result(FlutterError(code: "NOT_READY", message: "Model not loaded", details: nil))
      return
    }
    
    let args = call.arguments as? [String: Any] ?? [:]
    let sampleRate = args["sampleRate"] as? Int ?? 16000
    let channels = args["channels"] as? Int ?? 1
    let format = args["format"] as? String ?? "pcm16"
    
    // Apply configuration if provided
    if let config = args["config"] as? [String: Any] {
      updateConfigFromMap(config)
    }
    
    print("ðŸŽ¤ Starting audio capture - sampleRate: \(sampleRate), channels: \(channels), format: \(format)")
    print("ðŸ“‹ Voice activity threshold: \(voiceActivityThreshold), language: \(currentLanguage)")
    print("âœ… Stream handler available: \(eventSink != nil)")
    
    isCapturingAudio = true
    audioBuffer.removeAll()
    
    print("âœ… Audio capture started successfully")
    result(nil)
  }
  
  private func stopAudioCapture(_ result: @escaping FlutterResult) {
    print("ðŸ›‘ Stopping audio capture")
    isCapturingAudio = false
    audioBuffer.removeAll()
    result(nil)
  }
  
  private func processAudioChunk(_ call: FlutterMethodCall, result: @escaping FlutterResult) {
    guard let args = call.arguments as? [String: Any],
          let audioData = args["audioData"] as? FlutterStandardTypedData,
          isCapturingAudio,
          let llm = llmInference else {
      let errorMsg = "Audio capture not started, missing audio data, or model not loaded"
      print("âŒ \(errorMsg)")
      result(FlutterError(code: "NOT_READY", message: errorMsg, details: nil))
      return
    }
    
    // Apply configuration if provided
    if let config = args["config"] as? [String: Any] {
      updateConfigFromMap(config)
    }
    
    let floats = convertPcm16ToFloat32(audioData.data)
    audioBuffer.append(contentsOf: floats)
    
    print("ðŸ“Š Audio chunk processed: \(floats.count) samples, buffer total: \(audioBuffer.count)")
    
    // Calculate intervals based on sample rate
    let sampleRate = 16000
    let interimSamples = (interimResultIntervalMs * sampleRate) / 1000
    let finalSamples = (finalResultIntervalMs * sampleRate) / 1000
    
    // Process transcription when we have enough audio data
    if audioBuffer.count >= interimSamples {
      // Send speech result to stream if we have a listener
      if let eventSink = self.eventSink {
        print("ðŸŽ¤ Processing audio buffer with \(audioBuffer.count) samples")
        
        // TODO: Replace with actual Gemma 3 ASR when available
        // For now, simulate speech recognition results with configurable thresholds
        let audioLevel = calculateAudioLevel(audioBuffer)
        let hasVoiceActivity = audioLevel > voiceActivityThreshold
        
        if hasVoiceActivity {
          let speechResult: [String: Any] = [
            "type": "speechResult",
            "text": generateSimulatedSpeechText(audioLevel: audioLevel),
            "confidence": min(0.9, audioLevel * 10), // Scale audio level to confidence
            "isFinal": false,
            "timestamp": Int(Date().timeIntervalSince1970 * 1000),
            "audioLevel": audioLevel,
            "language": currentLanguage
          ]
          
          print("ðŸ“¤ Sending interim speech result: confidence=\(speechResult["confidence"] ?? 0)")
          DispatchQueue.main.async {
            eventSink(speechResult)
          }
        } else {
          print("ðŸ”‡ No significant voice activity detected (level: \(String(format: "%.3f", audioLevel)), threshold: \(voiceActivityThreshold))")
        }
        
        // Send final result based on configured interval
        if audioBuffer.count >= finalSamples {
          let avgLevel = calculateAudioLevel(audioBuffer)
          if avgLevel > finalResultThreshold {
            let finalResult: [String: Any] = [
              "type": "speechResult", 
              "text": generateSimulatedSpeechText(audioLevel: avgLevel, isFinal: true),
              "confidence": min(0.95, avgLevel * 12),
              "isFinal": true,
              "timestamp": Int(Date().timeIntervalSince1970 * 1000),
              "audioLevel": avgLevel,
              "language": currentLanguage
            ]
            
            print("âœ… Sending final speech result")
            DispatchQueue.main.async {
              eventSink(finalResult)
            }
          } else {
            print("ðŸ”‡ No final result sent - insufficient voice activity (level: \(String(format: "%.3f", avgLevel)), threshold: \(finalResultThreshold))")
          }
          
          // Clear buffer after final result
          print("ðŸ—‘ï¸ Clearing audio buffer (\(audioBuffer.count) samples)")
          audioBuffer.removeAll()
        }
      } else {
        print("âš ï¸ No event sink available for speech results")
      }
    }
    
    // Dynamic buffer size based on configuration
    let maxConfiguredBufferSize = (bufferSizeMs * 16000) / 1000
    if audioBuffer.count > maxConfiguredBufferSize {
      let keepSize = maxConfiguredBufferSize / 2
      print("âš ï¸ Audio buffer too large (\(audioBuffer.count)), trimming to \(keepSize)")
      audioBuffer = Array(audioBuffer.suffix(keepSize))
    }
    
    result(nil)
  }
  
  private func generateText(_ call: FlutterMethodCall, result: @escaping FlutterResult) {
    guard let args = call.arguments as? [String: Any],
          let prompt = args["prompt"] as? String,
          let llm = llmInference else {
      result(FlutterError(code: "INVALID_ARGUMENT", message: "Missing prompt or model not loaded", details: nil))
      return
    }
    
    do {
      let sessionOptions = createSessionOptions(args)
      let session = try MediaPipeTasksGenAI.LlmInference.Session(llmInference: llm, options: sessionOptions)
      try session.addQueryChunk(inputText: prompt)
      let response = try session.generateResponse()
      
      result([
        "success": true,
        "text": response
      ])
    } catch {
      result(FlutterError(code: "INFERENCE_FAILED", message: error.localizedDescription, details: nil))
    }
  }
  
  /// Update speech processing configuration
  private func updateSpeechConfig(_ call: FlutterMethodCall, result: @escaping FlutterResult) {
    guard let args = call.arguments as? [String: Any],
          let config = args["config"] as? [String: Any] else {
      result(FlutterError(code: "INVALID_ARGUMENT", message: "Missing config", details: nil))
      return
    }
    
    updateConfigFromMap(config)
    
    print("ðŸ“‹ Updated speech config: VAD=\(voiceActivityThreshold), Lang=\(currentLanguage)")
    result(nil)
  }
  
  /// Update configuration from map
  private func updateConfigFromMap(_ config: [String: Any]) {
    if let threshold = config["voiceActivityThreshold"] as? Double {
      voiceActivityThreshold = Float(threshold)
    }
    if let threshold = config["finalResultThreshold"] as? Double {
      finalResultThreshold = Float(threshold)
    }
    if let bufferMs = config["bufferSizeMs"] as? Int {
      bufferSizeMs = bufferMs
    }
    if let intervalMs = config["interimResultIntervalMs"] as? Int {
      interimResultIntervalMs = intervalMs
    }
    if let intervalMs = config["finalResultIntervalMs"] as? Int {
      finalResultIntervalMs = intervalMs
    }
    if let language = config["language"] as? String {
      currentLanguage = language
    }
    if let enabled = config["enableLanguageDetection"] as? Bool {
      enableLanguageDetection = enabled
    }
    if let enabled = config["enableRealTimeEnhancement"] as? Bool {
      enableRealTimeEnhancement = enabled
    }
  }
  
  /// Get ASR capabilities
  private func getASRCapabilities(_ result: @escaping FlutterResult) {
    result([
      "supportsRealTimeASR": false, // Will be true when Gemma 3 ASR is fully implemented
      "supportsLanguageDetection": enableLanguageDetection,
      "supportsConfigurableThresholds": true,
      "supportedLanguages": ["en", "es", "fr", "de", "it", "pt", "zh", "ja", "ko", "ar"],
      "currentLanguage": currentLanguage,
      "voiceActivityThreshold": voiceActivityThreshold,
      "finalResultThreshold": finalResultThreshold,
      "bufferSizeMs": bufferSizeMs
    ])
  }
  
  /// Generate simulated speech text based on audio characteristics
  private func generateSimulatedSpeechText(audioLevel: Float, isFinal: Bool = false) -> String {
    // TODO: Replace with actual Gemma 3 ASR transcription
    let intensity = audioLevel > 0.05 ? "strong" : audioLevel > 0.02 ? "moderate" : "weak"
    let type = isFinal ? "Final" : "Interim"
    
    // Simulate language-specific patterns
    switch currentLanguage {
    case "es":
      return "\(type) transcripciÃ³n detectada (\(intensity) nivel: \(String(format: "%.3f", audioLevel)))"
    case "fr":
      return "\(type) transcription dÃ©tectÃ©e (\(intensity) niveau: \(String(format: "%.3f", audioLevel)))"
    case "de":
      return "\(type) Transkription erkannt (\(intensity) Stufe: \(String(format: "%.3f", audioLevel)))"
    case "it":
      return "\(type) trascrizione rilevata (\(intensity) livello: \(String(format: "%.3f", audioLevel)))"
    case "pt":
      return "\(type) transcriÃ§Ã£o detectada (\(intensity) nÃ­vel: \(String(format: "%.3f", audioLevel)))"
    case "zh":
      return "\(type) æ£€æµ‹åˆ°è½¬å½• (\(intensity) çº§åˆ«: \(String(format: "%.3f", audioLevel)))"
    default: // en
      return "\(type) speech detected (\(intensity) level: \(String(format: "%.3f", audioLevel)))"
    }
  }
}
