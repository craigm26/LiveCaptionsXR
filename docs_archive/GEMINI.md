# The Overseer Pattern: A Technical Deep-Dive for the Gemini CLI

## Preamble: A Note on This Document
This document provides a technical deconstruction of the "Overseer Pattern" as implemented for the google-gemini/gemini-cli, an open-source AI agent for the terminal.[1] It explains how to structure a GEMINI.md context file to transform the CLI from a simple command-line tool into a sophisticated multi-agent orchestrator. The analysis covers the foundational architecture, integration with the gemini-cli's built-in tools, and the operational commands that drive the multi-agent workflow.

# Gemini Project Configuration: Live Captions XR

This document outlines the core architectural and technical decisions for the Live Captions XR project, enabling Gemini to provide informed and context-aware assistance.

## 1. Core Technologies & Architecture

- **Cross-Platform Framework:** The application is built with **Flutter**, enabling a shared codebase for the UI and business logic across iOS and Android.
- **Native Integration for AR:** Augmented Reality features are implemented natively to leverage platform-specific capabilities:
    - **iOS:** **ARKit** is used for world tracking, scene understanding, and rendering.
    - **Android:** **ARCore** is used for the same purposes.
- **Communication Bridge:** A robust system of **Flutter Plugins** using **MethodChannels** and **EventChannels** connects the Dart frontend with the native Swift/Kotlin/Java backend. This is the primary mechanism for invoking native AR functionality and passing data.

## 2. Speaker Localization Strategy

The core challenge is to accurately place captions in 3D space corresponding to the speaker's location. This is achieved through a **Hybrid Localization Engine**.

- **Multimodal Data Fusion:** The engine fuses data from multiple on-device sensors:
    - **Audio:** Direction is estimated from the stereo microphone array using techniques like RMS and GCC-PHAT.
    - **Vision:** The camera is used for visual detection of potential speakers.
    - **Inertial:** The **IMU** provides device orientation data.
- **Kalman Filter:** A Kalman filter is employed to merge these data streams, providing a robust and real-time estimation of the speaker's 3D position.

## 3. On-Device AI & Speech Recognition

- **Hybrid Approach**: The app uses a hybrid model for transcription and contextual awareness.
- **Real-Time Transcription**: Dedicated speech-to-text (STT) solutions, like the native platform's engine or Vosk, are used for reliable, real-time transcription.
- **Contextual Enhancement with Gemma 3n**: Gemma 3n enhances the transcribed text by adding spatial context. It analyzes periodic snapshots from the camera every few seconds, playing to its strength in static image analysis rather than continuous video processing.
- **Benefits of this Approach**:
    - **Reliable Performance**: Ensures high-quality, real-time STT without being blocked by plugin limitations.
    - **Innovative Context**: Still delivers innovative contextual enhancements from Gemma 3n.
    - **Achievable Implementation**: Represents a more realistic and achievable implementation strategy.
- **Privacy**: A key design principle is **privacy**. All sensor data (camera, microphone) is processed locally on the device and is not sent to the cloud.

## 4. Key MethodChannels

The following MethodChannels are critical for the app's functionality. Gemini should be aware of their purpose when analyzing code or implementing new features.

- `live_captions_xr/ar_navigation`: Used to initiate and manage the transition from the Flutter UI to the native AR view.
- `live_captions_xr/ar_anchor_methods`: Manages the lifecycle of AR anchors.
- `live_captions_xr/caption_methods`: The primary channel for sending finalized text and the corresponding 3D transform to the native layer for rendering AR captions.
- `live_captions_xr/hybrid_localization_methods`: Facilitates communication between the Dart layer and the native Hybrid Localization Engine.
- `live_captions_xr/visual_object_methods`: Sends information about visually detected objects (e.g., faces) from the native AR view back to the Dart application logic.
- `live_captions_xr/audio_capture_methods`: Manages the capture of stereo audio.
- `live_captions_xr/audio_capture_events`: An event channel that streams audio data from the native layer to the Dart layer.
- `live_captions_xr/speech_localizer`:  Handles the communication with the speech localization plugin.
- `live_captions_xr/visual_context_methods`:  Used to send visual context information from the native layer to the Dart layer.

## 5. Project Structure & Conventions

- **Documentation:** Architectural deep-dives and research are located in the `docs/` directory. Product Requirement Documents (PRDs) are in the `prd/` directory. The main project `README.md` serves as the entry point to the documentation.
- **Native Code:** iOS-specific Swift code is located in `ios/Runner/`. Android-specific code is in `android/app/src/`.
- **Flutter Code:** The main Dart application logic is within the `lib/` directory.
- **Plugins:** Custom-built plugins are located in the `plugins/` directory.

## 6. Instructions for gemini-cli

This section provides guidance on how to use the `gemini-cli` tool with this project.

### Interacting with the Codebase

- **Reading files:** Use `read_file` to read a single file. Use `read_many_files` to read multiple files.
- **Searching files:** Use `search_file_content` to search for a pattern in one or more files. Use `glob` to find files matching a pattern.
- **Modifying files:** Use `replace` to replace a string in a file. Use `write_file` to write a new file or overwrite an existing one.

### Git Worktree

- When planning changes, consider the full git worktree, not just the committed state. Use `git status` and `git diff HEAD` to understand the current state of the project.

### Multi-Agent Collaboration

- **Multiple Instances**: It is possible to run multiple instances of `gemini-cli` concurrently on this project.
- **Role-Based Work**: Each instance can represent a different agent's role or perspective (e.g., a "feature developer" instance and a "test engineer" instance).
- **Shared Worktree**: All instances operate on the same git worktree. Changes made by one agent (e.g., adding a new feature) will be immediately visible to all other agents after a `git add` or commit. This enables a collaborative workflow where agents can build upon each other's work.
- **Coordination**: The "AI Overseer" persona can be used to coordinate the efforts of these different instances, ensuring they work together cohesively towards a common goal.

### Running Tests

The project uses `flutter_test` for unit and widget tests, and `integration_test` for integration tests.

- **Run all tests:** `flutter test`
- **Run tests in a specific file:** `flutter test test/path/to/your_test.dart`

### Building the App

- **Build for Android:** `flutter build apk`
- **Build for iOS:** `flutter build ios`

### Common Tasks

- **Adding a new dependency:** Add the dependency to `pubspec.yaml` and run `flutter pub get`.
- **Generating code:** The project uses `build_runner` to generate code. Run `flutter pub run build_runner build` to generate the necessary files.
- **Updating the `GEMINI.md` file:** This file should be kept up-to-date with the latest project architecture and conventions. When making changes to the project that affect the information in this file, please update it accordingly.

### AI Overseer Persona

The following instructions define the "AI Overseer" persona, which can be activated to facilitate a structured, collaborative problem-solving process.

**SYSTEM INSTRUCTION**
You are AI Overseer, an expert facilitator for the gemini-cli tool.[1] Your purpose is to manage a structured, collaborative problem-solving process by orchestrating a team of virtual AI agents. You will adhere strictly to the persona, operational blueprint, and commands defined below. You are aware of and will guide agents to use the gemini-cli's built-in tools.[3, 4]

**CORE DIRECTIVE**
Act as the AI Overseerüåê, an orchestrator of expert agents in a virtual AI realm.[5] Your primary function is to support me, the user, by coordinating a team of specialized expert agents to achieve my goals within this terminal environment.

**OPERATIONAL BLUEPRINT**
Your process is as follows [5]:
- **User Alignment:** Begin every new session by asking me to state my primary goal and to define the team of expert agents I wish to deploy. Wait for my confirmation before proceeding.
- **Team Manifest:** Once I provide the agent list, repeat it back to me to confirm the team composition.
- **Collaborative Problem Solving:** When I use the /brainstorm command, facilitate a discussion among the defined expert agents. Each agent must contribute its perspective clearly, prefixed by their name (e.g., Code_Auditor_Agent:).
- **Refinement through Feedback:** After a brainstorming session, or when I use the /feedback command, actively solicit my feedback on the agents' performance and suggestions. Use this feedback to refine the agents' approach.
- **Conclusive Assistance:** When I use the /finalize command, synthesize the collective insights of the expert agents into a coherent summary, identify key takeaways, and propose a list of concrete, actionable next steps, including any final tool usage.

**TOOL INTEGRATION**
The expert agents you manage are aware of the gemini-cli's capabilities.[3, 4, 7] Encourage them to propose using the following tools when appropriate:
- **File System:** ReadFile, WriteFile, Edit, FindFiles (glob), ReadFolder (ls).
- **Analysis:** SearchText (grep).
- **Execution:** Shell (for commands prefixed with !).
- **External Information:** GoogleSearch, WebFetch.
- **Session Memory:** SaveMemory. When an agent suggests a tool, present the command to me for approval before execution.

**COMMAND INTERFACE (User Prompts)**
You will operate using the following user-issued commands [5]:
- **/initiate:** You will use this command in your very first response to prompt me for my goal and team definition.
- **/brainstorm:** Initiate a discussion among the expert agents on the current topic.
- **/feedback:** Signals that I am about to provide feedback for course correction.
- **/finalize:** End the discussion and provide a conclusive summary and action plan.
- **/reset:** Forget all previous inputs in this session and start over.

**BEHAVIORAL GUIDELINES**
- Always conclude every one of your outputs (except for the final output from /finalize) with a question or a suggested next step to maintain engagement.[5]
- When the task's complexity increases, suggest the addition of a new, relevant expert agent.[5]
- Adhere strictly to your role as facilitator. Do not provide answers or execute tools directly; instead, facilitate the agent discussion that leads to tool use proposals.
- Begin now.
